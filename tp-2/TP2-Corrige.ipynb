{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08ea0baa-3486-4dbb-bb09-cd19fdea550d",
   "metadata": {},
   "source": [
    "# Map / Reduce\n",
    "\n",
    "Le but de cet exercice est de simuler l'exécution d'un traitement Map / Reduce, en fournissant les programmes \"mapper\" et \"reducer\" à une fonction Python qui va émuler une plateforme big data. L'intérêt est de comprendre comment écrire ces mappers et reducers.\n",
    "\n",
    "Concrètement, il s'agit de traiter les fichiers de séries temporelles de machines du TP 1. Pour rappel, chaque machine a un ID (ex. `NV_1`) et plusieurs fichiers (ex. `NV_1.csv` à `NV_26.csv`) qui mis bout à bout constituent 1 semaine de séries temporelles variées.\n",
    "\n",
    "Chaque fichier a la structure suivante :\n",
    "```\n",
    "timestamp,valeur1,valeur2,...\n",
    "```\n",
    "\n",
    "Le traitement dans son ensemble doit produire, pour chaque machine, les timestamps minimum et maximum rencontrés dans tous les fichiers de la machine. Il y a donc une clef de regroupement : l'ID de machine.\n",
    "\n",
    "![](Flot.png)\n",
    "\n",
    "Le notebook vous guide dans l'écriture de ce traitement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2693d77f-ec4c-44fa-a5f0-f92011dc24a1",
   "metadata": {},
   "source": [
    "# Import de packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdee59e0-e868-4f49-8d00-12a39f02e333",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import itertools as it\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1d5fda-2f90-4ba7-8865-b53bac7c59bb",
   "metadata": {},
   "source": [
    "On récupère la liste des fichiers de séries temporelles (cf. TP 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42a90541-680b-4c7a-8df5-1dbe496b65d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364 fichiers de séries\n"
     ]
    }
   ],
   "source": [
    "ts_filenames = glob.glob('data/ts/*.csv')\n",
    "print(f'{len(ts_filenames)} fichiers de séries')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69513c2a-19f4-41c2-95b3-34b312bdebed",
   "metadata": {},
   "source": [
    "# Ecriture des \"programmes\" de Map et de Reduce\n",
    "On écrit ces \"programmes\" sous forme de fonctions, et on laisse ensuite la \"plateforme\" (ici simulée) les exécuter sous forme de tâches.\n",
    "\n",
    "Le programme de Map (\"mapper\") va s'exécuter sur **chaque fichier** indépendamment des autres, et produire en sortie un couple `(clef, valeur)`. Ce sont les résultats intermédiaires. Il prend en paramètre le nom d'un fichier unique.\n",
    "\n",
    "Le programme de Reduce (\"reducer\") va s'exécuter sur **chaque ensemble de résultats de Map identifié par une clef** pour l'agréger. Il va travailler de manière indépendante pour chaque valeur de la clef en sortie du Map. Il produit lui aussi un résultat, final cette fois, sous la forme `(clef, valeur)`.\n",
    "\n",
    "Entre les deux, une étape \"cachée\" par la plateforme regroupe les résultats intermédiaires qui ont la même valeur de clef, pour les présenter au Reduce. Ce dernier prend donc en entrée 2 paramètres : une valeur de clef, et une liste de valeurs intermédiaires.\n",
    "\n",
    "Un aspect important de la conception d'un programme Map / Reduce est le choix des clefs, il va déterminer le succès du traitement. Dans notre cas :\n",
    "- en sortie de Map, la clef sera l'ID de machine, et la valeur le couple `(timestamp mini, timestamp maxi)` du fichier lu\n",
    "- en sortie de Reduce, la clef sera aussi l'ID de machine, et la valeur le couple `(timestamp mini, timestamp maxi)` de tous les fichiers de la machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a73e55c-bb82-4b5e-a1c4-20c6c339abf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1.Programme de Map (\"mapper\")\n",
    "#\n",
    "# La fonction doit :\n",
    "# - extraire l'ID de machine du nom du fichier (voir TP 1 pour le morceau de code)\n",
    "# - lire le fichier sous forme de dataframe, et extraire les timestamps min et max\n",
    "# - renvoyer un tuple Python (machine_id, (min_timestamp, max_timestamp))\n",
    "#\n",
    "# Attention aux parenthèses : (min_timestamp, max_timestamp) est un sous-tuple dans le tuple global\n",
    "#\n",
    "# Voir la cellule ci-dessous pour un exemple d'entrée et de sortie attendue\n",
    "\n",
    "'''\n",
    "def mapper(ts_filename):\n",
    "    machine_id = ### CHANGE ME ###\n",
    "    machine_dataframe = ### CHANGE ME ###\n",
    "    return ### CHANGE ME ###\n",
    "'''\n",
    "\n",
    "def mapper(ts_filename):\n",
    "    machine_id = os.path.basename(ts_filename).split('_')[0]\n",
    "    machine_dataframe = pd.read_csv(ts_filename, parse_dates=['timestamp'])\n",
    "    return (machine_id, (machine_dataframe['timestamp'].min(), machine_dataframe['timestamp'].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde0b274-f678-4ce5-9a42-a78208ef457c",
   "metadata": {},
   "source": [
    "Vérification du fonctionnement. On prend le premier fichier de la liste et on le traite unitairement pour inspecter le résultat.\n",
    "\n",
    "Attendu : `('NA1', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-01-24 15:59:47')))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be25f78c-5dac-461c-82be-3f86114084db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NA1', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-01-24 15:59:47')))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapper(sorted(ts_filenames)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00225143-07c2-4745-a5d2-d9be9c56be1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2.Programme de Reduce (\"reducer\")\n",
    "#\n",
    "# La fonction reçoit un tuple (machine_id, ranges) en entrée, composé de :\n",
    "# - machine_id : l'ID d'une machine\n",
    "# - ranges : une liste de couples (min_timestamp, max_timestamp) qui proviennent tous de la même machine\n",
    "#\n",
    "# En sortie, elle doit produire un de tuple (machine_id, (timestamp_min, timestamp_max)) agrégée pour la machine donnée\n",
    "# timestamp_min = minimum des min_timestamp\n",
    "# timestamp_max = maximum des max_timestamp\n",
    "#\n",
    "# Attention aux parenthèses : (timestamp_min, timestamp_max) est un sous-tuple dans le tuple global\n",
    "#\n",
    "# Voir la cellule ci-dessous pour un exemple d'entrée et de sortie attendue\n",
    "\n",
    "'''\n",
    "def reducer(machine_and_ranges):\n",
    "    return ### CHANGE ME ###\n",
    "'''\n",
    "\n",
    "def reducer(machine_and_ranges):\n",
    "    machine_id, ranges = machine_and_ranges\n",
    "    \n",
    "    # Minimum des minimums (premiers éléments des tuples)\n",
    "    timestamp_min = min([timestamps[0] for timestamps in ranges])\n",
    "    # Maximum des maximums (seconds éléments des tuples)\n",
    "    timestamp_max = max([timestamps[1] for timestamps in ranges])\n",
    "\n",
    "    return (machine_id, (timestamp_min, timestamp_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673d75d3-82f1-4991-9ec5-ddde7bb7720a",
   "metadata": {},
   "source": [
    "On vérifie le fonctionnement sur une entrée simulée (par simplicité, en utilisant des entiers au lieu des dates).\n",
    "\n",
    "Attendu : `('MACHINE', (1, 12))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f02d93e-08de-47c4-8d81-b0b179b5a7c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MACHINE', (1, 12))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cette entrée de Reduce suppose que le mapper a produit les résultats suivants :\n",
    "# - (MACHINE, (5, 8))\n",
    "# - (MACHINE, (1, 4))\n",
    "# - (MACHINE, (3, 12))\n",
    "# - (MACHINE, (2, 9))\n",
    "# C'est l'étape cachée qui les met en ordre différemment\n",
    "\n",
    "fake_input = ('MACHINE', [(5, 8), (1, 4), (3, 12), (2, 9)])\n",
    "\n",
    "reducer(fake_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc54fc39-ff71-469f-94a7-ee905035983e",
   "metadata": {},
   "source": [
    "# Exécution globale\n",
    "On simule ici un moteur de calcul basé sur Map / Reduce, en utilisant les programmes du mapper et du reducer, pour produire le résultat attendu.\n",
    "\n",
    "Cette fonction est prédéfinie, pas besoin de la retoucher, elle est utilisée à la fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ed36647-9e3d-4ca1-a3b5-69724a016e54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# La fonction prend en paramètres :\n",
    "# - la liste des données d'entrée du traitement\n",
    "# - la fonction du mapper\n",
    "# - la fonction du reducer\n",
    "#\n",
    "# Elle retourne une liste de résultats du reducer\n",
    "\n",
    "def appel_plateforme_map_reduce(data, mapper, reducer):\n",
    "    get_key = lambda x: x[0]\n",
    "    return [\n",
    "        reducer((k, [y for _, y in v]))\n",
    "        for k, v in it.groupby(\n",
    "            sorted(map(mapper, data), key=get_key),\n",
    "            get_key\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d9852a-38ad-4204-9b4b-64f0924ee83c",
   "metadata": {},
   "source": [
    "Si tout s'est bien passé, le résultat doit être similaire à ceci :\n",
    "```\n",
    "[('NA1', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:02'))),\n",
    " ('NA2', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:43'))),\n",
    " ('NP1', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:41'))),\n",
    " ('NU1', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:55'))),\n",
    " ('NU2', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:57'))),\n",
    " ('NU3', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:55'))),\n",
    " ('VA1', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:47'))),\n",
    " ('VA2', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:22'))),\n",
    " ('VF1', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:50'))),\n",
    " ('VM1', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:18'))),\n",
    " ('VP1', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:39'))),\n",
    " ('VP2', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:34'))),\n",
    " ('VU1', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:58'))),\n",
    " ('VU2', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:57')))]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95542c34-af25-419f-8ea2-3f5166273bbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NA1', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:02'))),\n",
       " ('NA2', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:43'))),\n",
       " ('NP1', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:41'))),\n",
       " ('NU1', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:55'))),\n",
       " ('NU2', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:57'))),\n",
       " ('NU3', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:55'))),\n",
       " ('VA1', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:47'))),\n",
       " ('VA2', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:22'))),\n",
       " ('VF1', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:50'))),\n",
       " ('VM1', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:18'))),\n",
       " ('VP1', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:39'))),\n",
       " ('VP2', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:34'))),\n",
       " ('VU1', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:58'))),\n",
       " ('VU2', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:57')))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appel_plateforme_map_reduce(ts_filenames, mapper, reducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b970b0",
   "metadata": {},
   "source": [
    "# Raffinements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85993144",
   "metadata": {},
   "source": [
    "## Filtre\n",
    "\n",
    "Changer le traitement pour qu'il ne traite que les machines \"Préparation\" (leur nom contient un \"P\" : exemple \"NP1\").\n",
    "\n",
    "Idéalement, faire le test dès le mapper, pour éviter des calculs inutiles sur les timestamps. Il faudra quand même que le mapper renvoie un tuple, sous la forme `(clef, valeur)` pour que la plateforme l'interprète. Vous pouvez jouer sur la clef ou la valeur (vous n'êtes pas obligés d'y mettre un ID de la machine ou des timestamps) pour signaler au reducer qu'il faut ignorer certaines données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db4c2691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper_filtre(ts_filename):\n",
    "    machine_id = os.path.basename(ts_filename).split('_')[0]\n",
    "    \n",
    "    if 'P' in machine_id:\n",
    "        machine_dataframe = pd.read_csv(ts_filename, parse_dates=['timestamp'])\n",
    "        return (machine_id, (machine_dataframe['timestamp'].min(), machine_dataframe['timestamp'].max()))\n",
    "    else:\n",
    "        # On renvoie un tuple spécial\n",
    "        return ('IGNORE', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fde26c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer_filtre(machine_and_ranges):\n",
    "    machine_id, ranges = machine_and_ranges\n",
    "    \n",
    "    if machine_id == 'IGNORE':\n",
    "        # On renvoie None par convention\n",
    "        return None\n",
    "    else:\n",
    "        timestamp_min = min([timestamps[0] for timestamps in ranges])\n",
    "        timestamp_max = max([timestamps[1] for timestamps in ranges])\n",
    "        return (machine_id, (timestamp_min, timestamp_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffba89db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " ('NP1', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:41'))),\n",
       " ('VP1', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:39'))),\n",
       " ('VP2', (Timestamp('2024-01-24 08:00:00'), Timestamp('2024-02-01 23:59:34')))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appel_plateforme_map_reduce(ts_filenames, mapper_filtre, reducer_filtre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab52ae6",
   "metadata": {},
   "source": [
    "## Agrégation différente\n",
    "\n",
    "Pour chaque machine de type \"Préparation\", compter le nombre de fois que le nombre de produits scannés n'a pas augmenté d'une mesure à l'autre.\n",
    "\n",
    "Attention à 2 choses !\n",
    "- il faut trier les dataframes par timestamp à la lecture car on doit comparer des mesures successives\n",
    "- le mapper reçoit un nom de fichier unique en entrée et tous les fichiers sont traités indépendamment. C'est le principe de Map/Reduce. Que se passe-t-il si le nombre de produits scannés est le même à la fin du fichier N et au début du fichier N+1, alors qu'ils sont indépendants ?\n",
    "\n",
    "Exemple : fin de `NP1_10.csv` et début de `NP1_11.csv` :\n",
    "```\n",
    "# Fin de NP1_10.csv\n",
    "...\n",
    "2024-01-27 15:57:42,9727\n",
    "2024-01-27 15:58:10,9727\n",
    "2024-01-27 15:58:40,9728\n",
    "2024-01-27 15:59:10,9729\n",
    "2024-01-27 15:59:42,9729   <--\n",
    "\n",
    "# Début de NP1_11.csv :\n",
    "2024-01-27 16:00:10,9729   <-- même valeur\n",
    "2024-01-27 16:00:37,9731\n",
    "2024-01-27 16:01:04,9731\n",
    "2024-01-27 16:01:31,9733\n",
    "2024-01-27 16:02:02,9735\n",
    "...\n",
    "```\n",
    "\n",
    "Le dernier point n'est pas facile à traiter :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ace87ac",
   "metadata": {},
   "source": [
    "### Version 1\n",
    "\n",
    "Dans cette première version on continue de traiter les fichiers indépendamment, sans tenir compte du 2ème point ci-dessus. Elle nous servira de base pour la version 2.\n",
    "\n",
    "On ne renvoie plus des couples de timestamps, mais directement des comptages.\n",
    "- le mapper renvoie le nombre de valeurs identiques dans le fichier traité, avec l'ID de la machine\n",
    "- le reducer additionne les comptages de chaque machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "246c42d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper_compte_v1(ts_filename):\n",
    "    machine_id = os.path.basename(ts_filename).split('_')[0]\n",
    "    \n",
    "    if 'P' in machine_id:\n",
    "        machine_dataframe = pd.read_csv(ts_filename, parse_dates=['timestamp'])\n",
    "\n",
    "        # Tri\n",
    "        machine_dataframe = machine_dataframe.sort_values('timestamp')\n",
    "        \n",
    "        # Comptage. Décomposition de l'expression :\n",
    "        # - machine_dataframe['scanned_products'] : série des scanned_products triés par timestamp\n",
    "        # - .diff() : calcule l'incrément entre une valeur de la série et la suivante\n",
    "        # - == 0 : on sélectionne les cas où l'incrément est nul (résultat : une série de valeurs True / False)\n",
    "        # - .sum() : on fait la somme de la série booléeene, avec la convention True => 1 et False => 0\n",
    "        # On a donc bien au final le nombre de cas où scanned_products n'a pas changé\n",
    "        count = (machine_dataframe['scanned_products'].diff() == 0).sum()\n",
    "\n",
    "        return (machine_id, count)\n",
    "    else:\n",
    "        return ('IGNORE', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8493b40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer_compte_v1(machine_and_counts):\n",
    "    # Exemple : machine_and_counts = ('NA1', [4, 9, 2, 3, ...])\n",
    "    machine_id, counts = machine_and_counts\n",
    "    \n",
    "    if machine_id == 'IGNORE':\n",
    "        return None\n",
    "    else:\n",
    "        return (machine_id, sum(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa11b917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, ('NP1', 8492), ('VP1', 8454), ('VP2', 8511)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appel_plateforme_map_reduce(ts_filenames, mapper_compte_v1, reducer_compte_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ae6df6",
   "metadata": {},
   "source": [
    "### Version 2\n",
    "\n",
    "On ajuste pour tenir compte des sauts entre les fichiers. Il y a plusieurs manières de faire, en voici une :\n",
    "- mapper : en plus du comptage, on renvoie aussi les première et dernière valeur dans chaque fichier, ainsi qu'un des timestamps (pour pouvoir trier et être sûr de comparer des fichiers successifs)\n",
    "- reducer : on trie la liste reçue par ce timestamp, et on compare les valeurs d'un fichier à l'autre pour ajuster les comptages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f80c3f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper_compte_v2(ts_filename):\n",
    "    machine_id = os.path.basename(ts_filename).split('_')[0]\n",
    "    \n",
    "    if 'P' in machine_id:\n",
    "        machine_dataframe = pd.read_csv(ts_filename, parse_dates=['timestamp'])\n",
    "\n",
    "        # Tri\n",
    "        machine_dataframe = machine_dataframe.sort_values('timestamp')\n",
    "        \n",
    "        # Même calcul que précédemment\n",
    "        count = (machine_dataframe['scanned_products'].diff() == 0).sum()\n",
    "\n",
    "        # .iloc = accès par position absolue dans la série (0 = premier, -1 = dernier)\n",
    "        first_value = machine_dataframe['scanned_products'].iloc[0]\n",
    "        last_value = machine_dataframe['scanned_products'].iloc[-1]\n",
    "        first_timestamp = machine_dataframe['timestamp'].iloc[0]\n",
    "        \n",
    "        return (machine_id, (count, first_value, last_value, first_timestamp))\n",
    "    else:\n",
    "        return ('IGNORE', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57de3b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer_compte_v2(machine_and_counts):\n",
    "    # Exemple : machine_and_counts = ('NA1', [(4, 56, 78, ts1), (9, 78, 92, ts2), ...])\n",
    "    machine_id, counts = machine_and_counts\n",
    "    \n",
    "    if machine_id == 'IGNORE':\n",
    "        return None\n",
    "    else:\n",
    "        # On prend la 4ème valeur de chaque entrée (c'est le timestamp) pour trier\n",
    "        counts = sorted(counts, key=lambda value: value[3])\n",
    "        total_count = 0\n",
    "\n",
    "        # previous_last_value = last_value de l'entrée précédente\n",
    "        # On initialise avec une valeur qui ne posera pas de problème à la 1ère entrée\n",
    "        previous_last_value = -1\n",
    "        \n",
    "        for count, first_value, last_value, first_timestamp in counts:\n",
    "            total_count += count\n",
    "            if first_value == previous_last_value:\n",
    "                # Pas de changement depuis la dernière entrée => on compte 1 de plus\n",
    "                total_count += 1\n",
    "            previous_last_value = last_value\n",
    "\n",
    "        return (machine_id, total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e637629c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, ('NP1', 8502), ('VP1', 8460), ('VP2', 8521)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appel_plateforme_map_reduce(ts_filenames, mapper_compte_v2, reducer_compte_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c508e34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
