{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d13b4c1-427a-4741-afcc-92c94545fa96",
   "metadata": {},
   "source": [
    "# Description du cas\n",
    "\n",
    "Cet exercice a pour but la manipulation de données structurées au format Parquet.\n",
    "\n",
    "Nous allons traiter les données d'une entreprise industrielle fictive, Blabla Inc. Les données concernent 2 **sites** (usines) de l'entreprise : Naves et Villemomble.\n",
    "\n",
    "Chaque site comporte plusieurs **ateliers** (_workshops_), et chaque atelier possède une ou plusieurs **machines**.\n",
    "\n",
    "Enfin, chaque machine enregistre son état à une fréquence donnée, sous forme de **séries temporelles**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5546c2d-34db-4bd1-aa9a-343797e3973c",
   "metadata": {},
   "source": [
    "## Ce qui est demandé\n",
    "\n",
    "Il est demandé de lire les données d'entrée (JSON et CSV), de les traiter en mémoire, et de les mettre en forme avant de les stocker au **format Parquet** selon un modèle attendu.\n",
    "\n",
    "Lorsque vous voyez un marqueur du type `### CHANGE ME ###` dans le code, ou dans un commentaire, c'est qu'il y a quelque chose à compléter pour accomplir une tâche.\n",
    "\n",
    "Le notebook va vous guider dans cette tâche. Voyons d'abord les données à manipuler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb6e406-d7dd-4805-9b3f-0b35e518be6d",
   "metadata": {},
   "source": [
    "## Les données\n",
    "\n",
    "Les données se trouvent dans le répertoire `data/` de votre notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ca5beb-2de9-4e81-9c95-012fe917b322",
   "metadata": {},
   "source": [
    "### L'organisation des sites et ateliers\n",
    "\n",
    "L'organisation de Blabla Inc. est décrite de manière hiérarchique dans le fichier `equipments.json`, qui contient un objet `organization` avec la structure suivante :\n",
    "- `name` : nom de l'entreprise\n",
    "- `sites` : liste des sites de l'entreprise, avec pour chacun :\n",
    "  - `name` : nom du site\n",
    "  - `workshops` : liste des ateliers, avec pour chacun :\n",
    "    - `name` : nom de l'atelier\n",
    "    - `machines` : liste des ID de machines présentes dans l'atelier (de 1 à 3 machines selon le site et l'atelier)\n",
    "\n",
    "<u>Extrait du fichier :</u>\n",
    "```json\n",
    "{\n",
    "  \"organization\": {\n",
    "    \"name\": \"Blabla Inc.\",\n",
    "    \"sites\": [\n",
    "      {\n",
    "        \"name\": \"Naves\",\n",
    "        \"workshops\": [\n",
    "          {\"name\": \"Préparation\", \"machines\": [\"NP1\"]},\n",
    "          {\"name\": \"Usinage\", \"machines\": [\"NU1\", \"NU2\", \"NU3\"]},\n",
    "          ...\n",
    "        ]\n",
    "      },\n",
    "      ...\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa0dd1b-8a10-431a-a7b1-17429ed9a43f",
   "metadata": {},
   "source": [
    "### Les \"mappings\" de noms de machines\n",
    "\n",
    "Les ID de machines qui sont dans le fichier d'organisation, sont des noms de code. Dans l'exercice, il est demandé de mettre à la place les noms \"lisibles\" des machines ; pour cela il faut une correspondance.\n",
    "\n",
    "La table de correspondance est dans le fichier `mapping.csv`, qui liste toutes les machines avec pour chacune :\n",
    "- `machine_id` : ID codé de la machine (même nomenclature que dans `equipments.json`)\n",
    "- `machine_name` : nom lisible de la machine (celui à reprendre dans les fichier de sortie)\n",
    "\n",
    "<u>Extrait du fichier :</u>\n",
    "```\n",
    "machine_id;machine_name\n",
    "NP1;Golgoth 3000\n",
    "NU1;Fraisator-1\n",
    "NU2;Fraisator-2\n",
    "NU3;Fraisator-3\n",
    "NA1;Lego_A\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace5fd98-9ad7-4819-93e0-5933eb923a89",
   "metadata": {},
   "source": [
    "### Les séries temporelles\n",
    "\n",
    "Les séries sont des groupes de fichiers, dans le répertoire `data/ts/`, avec plusieurs fichiers par machine. Les fichiers sont nommés `ID_n.csv`, où `ID` est l'ID de machine, et `n` le numéro de séquence du fichier, de 1 à 26 : chaque série est découpée par blocs de 8h.\n",
    "\n",
    "La structure de chaque fichier CSV est la suivante :\n",
    "- `timestamp` : la date et l'heure du point de mesure (format aaaa-mm-jj hh:mm:ss)\n",
    "- un champ par type de mesure, en fonction de l'atelier où se trouve la machine :\n",
    "  - \"Préparation\" : `scanned_products` est un entier donnant le nombre de produits pointés à l'étape de préparation\n",
    "  - \"Usinage\" : `rotation_x`, `rotation_y` et `rotation_z` sont 3 flottants indiquant l'orientation de la tête d'usinage, en degrés\n",
    "  - \"Assemblage\" : `products` est un entier donnant le nombre de produits assemblés\n",
    "  - \"Four\" : `temperature` est la température du four (en °C), `humidity` son degré d'hygrométrie (en %)\n",
    "  - \"Métrologie\" : `products` est un entier donnant le nombre de produits finis contrôlés, et `width`, `length` et `height` (en cm) le résultat de la dernière mesure\n",
    "\n",
    "Noter que les timestamps sont irréguliers, et que les fréquences ne sont pas les mêmes d'un atelier à l'autre.\n",
    "\n",
    "Tous les fichiers ont les noms de champs en en-tête.\n",
    "\n",
    "<u>Exemple pour un four :</u>\n",
    "```\n",
    "timestamp,temperature,humidity\n",
    "2024-01-28 00:00:07,812,93\n",
    "2024-01-28 00:00:17,812,93\n",
    "2024-01-28 00:00:27,812,93\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae17de57-f166-459e-8c25-62dca7e1a8c8",
   "metadata": {},
   "source": [
    "# Imports de packages Python\n",
    "Nous aurons besoin de ces packages pour manipuler les fichiers.\n",
    "\n",
    "- `pandas` : manipulation de dataframes en mémoire\n",
    "- `json` : pour la lecture des données JSON\n",
    "- `glob` : énumération des fichiers de données dans un répertoire\n",
    "- `os` : nous utiliserons ici les fonctions de manipulation de chemins de fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce3f938b-f4ea-4157-a915-92031ac1ae33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb4459e-89f2-4669-b367-e1617b8a327b",
   "metadata": {},
   "source": [
    "# Lecture des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcc208b-26cd-44ea-b9d5-65241c2ac732",
   "metadata": {},
   "source": [
    "## Organisation\n",
    "C'est un fichier JSON que nous pouvons lire directement, ce qui donne un dictionnaire Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3b4c24e-3596-4993-9848-a47b18a324e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'organization': {'name': 'Blabla Inc.',\n",
       "  'sites': [{'name': 'Naves',\n",
       "    'workshops': [{'name': 'Préparation', 'machines': ['NP1']},\n",
       "     {'name': 'Usinage', 'machines': ['NU1', 'NU2', 'NU3']},\n",
       "     {'name': 'Assemblage', 'machines': ['NA1', 'NA2']}]},\n",
       "   {'name': 'Villemomble',\n",
       "    'workshops': [{'name': 'Préparation', 'machines': ['VP1', 'VP2']},\n",
       "     {'name': 'Usinage', 'machines': ['VU1', 'VU2']},\n",
       "     {'name': 'Four', 'machines': ['VF1']},\n",
       "     {'name': 'Assemblage', 'machines': ['VA1', 'VA2']},\n",
       "     {'name': 'Métrologie', 'machines': ['VM1']}]}]}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/equipments.json', 'r') as f:\n",
    "    equipments = json.load(f)\n",
    "    \n",
    "equipments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb20f41-7780-4d5a-b911-324a2a0f99f4",
   "metadata": {},
   "source": [
    "## Mapping des noms de machines\n",
    "C'est un fichier CSV, mais nous avons besoin de le transformer en dictionnaire donnant, pour chaque ID de machine, son nom. Nous l'enrichissons ensuite avec d'autres infos de l'équipement, qui indiquent où se trouve la machine (site, atelier).\n",
    "\n",
    "Nous utilisons la fonction `read_csv()` de Pandas, qui permet de lire un fichier à partir de son nom / chemin, en précisant le caractère (`sep`) qui sépare les champs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cfef0f-7d71-4ba0-9217-cea34394ef2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Lecture du fichier sous forme de dataframe\n",
    "#\n",
    "# Attendu : un dataframe de la forme (ordre des lignes indifférent, et ne pas tenir compte de l'index de ligne (nombres en gras) qui s'afficheront en sortie) :\n",
    "#\n",
    "# +------------+--------------+\n",
    "# | machine_id | machine_name |\n",
    "# +------------+--------------+\n",
    "# |        NP1 | Golgoth 3000 |\n",
    "# |        NU1 |  Fraisator-1 |\n",
    "# |        NU2 |  Fraisator-2 |\n",
    "# |        ... |          ... |\n",
    "# |        VM1 |     Sherlock |\n",
    "# +------------+--------------+\n",
    "\n",
    "mapping_df = pd.read_csv('### CHANGE ME ###', sep='### CHANGE ME ###')\n",
    "\n",
    "mapping_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ecf6f2-3058-4334-890a-7e2502251bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Transformation en dictionnaire Python, en itérant sur les lignes du dataframe\n",
    "# On fait un dictionnaire de petits dictionnaires {'machine_name': xxx}, pour pouvoir l'enrichir dans l'étape d'après\n",
    "#\n",
    "# Attendu : un dictionnaire de la forme :\n",
    "#\n",
    "# {'NP1': {'machine_name': 'Golgoth 3000'},\n",
    "#  'NU1': {'machine_name': 'Fraisator-1'},\n",
    "#  'NU2': {'machine_name': 'Fraisator-2'},\n",
    "#  'NU3': {'machine_name': 'Fraisator-3'},\n",
    "#  'NA1': {'machine_name': 'Lego_A'},\n",
    "#  ...\n",
    "#  'VP2': {'machine_name': 'Cleanomatic-2'}]\n",
    "\n",
    "mapping = {\n",
    "    row['machine_id']: { ### CHANGE ME ### }\n",
    "    for _, row in mapping_df.iterrows()\n",
    "}\n",
    "\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a27b2c-f5f2-4c90-a1b1-bdd6fd806d48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Enrichissement avec les données d'équipement\n",
    "# A l'issue de cette étape, chaque ID de machine est associé à toutes les infos de la machine\n",
    "#\n",
    "# Attendu : un dictionnaire de la forme :\n",
    "#\n",
    "# {'NP1': {'machine_name': 'Golgoth 3000', 'site': 'Naves', 'workshop': 'Préparation'},\n",
    "#  'NU1': {'machine_name': 'Fraisator-1', 'site': 'Naves', 'workshop': 'Usinage'},\n",
    "#  'NU2': {'machine_name': 'Fraisator-2', 'site': 'Naves', 'workshop': 'Usinage'},\n",
    "#  'NU3': {'machine_name': 'Fraisator-3', 'site': 'Naves', 'workshop': 'Usinage'},\n",
    "#  'NA1': {'machine_name': 'Lego_A', 'site': 'Naves', 'workshop': 'Assemblage'},\n",
    "#  ...\n",
    "#  'VM1': {'machine_name': 'Sherlock', 'site': 'Villemomble', 'workshop': 'Métrologie'}}\n",
    " \n",
    "for site in equipments['### CHANGE ME ###']['### CHANGE ME ###']:\n",
    "    for workshop in site['### CHANGE ME ###']:\n",
    "        for machine_id in workshop['### CHANGE ME ###']:\n",
    "            mapping_machine = mapping[machine_id]\n",
    "            ### CHANGE ME ### (ajouter le nom du site, le nom du workshop)\n",
    "\n",
    "mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d6b2c3-c939-4478-9591-4a03f2d8b4e7",
   "metadata": {},
   "source": [
    "## Séries temporelles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd821b54-2dd7-4532-a35e-962d7c0e28e4",
   "metadata": {},
   "source": [
    "Les séries sont dans plusieurs fichiers CSV, qu'on va regrouper sous forme d'un unique dataframe par machine (repérée par son ID, présent dans le nom de chaque fichier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e879ec-2e05-47bb-8522-556a0216d829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Enumération des fichiers de séries, dans le répertoire correspondant\n",
    "ts_filenames = glob.glob('data/ts/*.csv')\n",
    "print(f'{len(ts_filenames)} fichiers de séries')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba033c15-4972-481b-b73b-cd1eb266d39c",
   "metadata": {},
   "source": [
    "Ceci vous sera utile pour écrire le bout de code manquant :\n",
    "- un paramètre supplémentaire à la fonction `read_csv()` : `parse_dates`\n",
    "  - quand le fichier contient des dates, il faut aider Pandas à les détecter en indiquant les noms des champs concernés\n",
    "  - concrètement, il vous faudra ajouter un paramètre `parse_dates=['nom de champ']` pour lui dire quoi interpréter\n",
    "- la fonction `concat()` de Pandas (`pd.concat()`) qui permet de mettre bout à bout 2 dataframes pour en créer un unique. On lui passe en paramètre une liste de dataframes à concaténer, ex. `[df1, df2, df3, ...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce830f78-5ffa-471c-a9ca-6d6ef79f9a73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Agrégation des séries. Pour chaque fichier CSV de série temporelle :\n",
    "# - extraction de l'ID de machine, à partir du nom du fichier\n",
    "# - lecture du fichier CSV dans un petit dataframe\n",
    "# - concaténation avec le dataframe précédent de la machine\n",
    "# Le tout est stocké dans un dictionnaire de dataframes, `dataframes_per_machine`\n",
    "#\n",
    "# Type de sortie attendue à la fin (ordre indifférent) :\n",
    "#\n",
    "# Machine NP1: 25397 échantillons\n",
    "# Machine NU1: 166442 échantillons\n",
    "# Machine NU2: 166357 échantillons\n",
    "# Machine NU3: 166364 échantillons\n",
    "# ...\n",
    "# Machine VM1: 9423 échantillons\n",
    "#\n",
    "# +---------------------+-------------+------------+------------+\n",
    "# |           timestamp | rotationx_x | rotation_y | rotation_z |\n",
    "# +---------------------+-------------+------------+------------+\n",
    "# | 2024-01-29 16:00:03 |   88.763247 | -49.428614 | 133.959195 |\n",
    "# | 2024-01-29 16:00:08 |   90.583300 | -49.193791 | 134.367569 |\n",
    "# | 2024-01-29 16:00:13 |   88.727491 | -52.121831 | 136.238264 |\n",
    "# |                 ... |         ... |        ... |        ... |\n",
    "# | 2024-01-26 15:59:56 | -174.516082 | 52.336152  |  79.645205 |\n",
    "# +---------------------+-------------+------------+------------+\n",
    "\n",
    "\n",
    "# Initialise le dictionnaire avec des dataframes vides\n",
    "dataframes_per_machine = {\n",
    "    machine_id: pd.DataFrame()\n",
    "    for machine_id in mapping\n",
    "}\n",
    "\n",
    "\n",
    "# Parcourt les fichiers\n",
    "for filename in ts_filenames:\n",
    "    # Extraction de l'ID de machine\n",
    "    machine_id = os.path.basename(filename).split('_')[0]\n",
    "          \n",
    "    small_dataframe = ### CHANGE ME ###\n",
    "    dataframes_per_machine[machine_id] = ### CHANGE ME ###\n",
    "\n",
    "for machine_id, machine_dataframe in dataframes_per_machine.items():\n",
    "    print(f'Machine {machine_id}: {len(machine_dataframe)} échantillons')\n",
    "    \n",
    "dataframes_per_machine['NU2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955ecc88-1472-4dc8-8beb-68d59ad84b4e",
   "metadata": {},
   "source": [
    "Pour ajouter une nouvelle colonne à un dataframe `df`, on peut utiliser la syntaxe suivante : `df['nouvelle_colonne'] = valeur`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc30022-0557-4ef0-b639-cb1ffa6b369e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Ajout au dataframe des infos de nom de machine, de site et d'atelier\n",
    "#\n",
    "# Attendu un dataframe enrichi :\n",
    "# +---------------------+-------------+------------+------------+--------------+-------+----------+\n",
    "# |           timestamp | rotationx_x | rotation_y | rotation_z | machine_name |  site | workshop |\n",
    "# +---------------------+-------------+------------+------------+--------------+-------+----------+\n",
    "# | 2024-01-29 16:00:03 |   88.763247 | -49.428614 | 133.959195 |  Fraisator-2 | Naves | Usinage  |\n",
    "# | 2024-01-29 16:00:08 |   90.583300 | -49.193791 | 134.367569 |  Fraisator-2 | Naves | Usinage  |\n",
    "# | 2024-01-29 16:00:13 |   88.727491 | -52.121831 | 136.238264 |  Fraisator-2 | Naves | Usinage  |\n",
    "# |                 ... |         ... |        ... |        ... |  Fraisator-2 | Naves | Usinage  |\n",
    "# | 2024-01-26 15:59:56 | -174.516082 | 52.336152  |  79.645205 |  Fraisator-2 | Naves | Usinage  |\n",
    "# +---------------------+-------------+------------+------------+--------------+-------+----------+\n",
    "\n",
    "for machine_id, df in dataframes_per_machine.items():\n",
    "    ### CHANGE ME ###\n",
    "    \n",
    "dataframes_per_machine['NU2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6eb36c-a610-4bf2-a921-9c82e538699f",
   "metadata": {},
   "source": [
    "# Ecriture du résultat brut\n",
    "On sauvegarde chaque dataframe de machine dans le répertoire de sortie, en itérant sur le dictionnaire créé précédemment.\n",
    "\n",
    "Pandas a des fonctions `read_xxx()` (ex. `read_csv()`) pour la lecture, et à l'inverse, des fonctions `to_xxx()` (ex. `to_parquet()`) pour les écritures de dataframes en mémoire.\n",
    "\n",
    "Le paramètre `index=False` empêche l'écriture des index de lignes (nombres en gras) dans le fichier, car dans notre cas ils n'ont pas de signification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74875e5a-f36b-44c9-bee5-82850a2f0af5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for machine_id, machine_dataframe in dataframes_per_machine.items():\n",
    "    machine_dataframe.to_parquet(f'data/ts_parquet/{machine_id}.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbb4335-0cc0-4370-b477-9e72d96583b9",
   "metadata": {},
   "source": [
    "Vous pouvez consulter le répertoire `data/ts_parquet` dans le bandeau de gauche, pour constater que les fichiers ont bien été créés (un par machine)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99df7b0a-610a-4d76-92a3-61451b2b6ee4",
   "metadata": {},
   "source": [
    "# Ecriture du résultat - avec partitionnement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9a496e-41f4-4c28-bc49-0e955a9a1cb2",
   "metadata": {},
   "source": [
    "Ci-dessous, on extrait le jour du timestamp :\n",
    "- la fonction `.dt.strftime(format)` transforme un champ date de Pandas en chaîne de caractères, en appliquant un format de représentation\n",
    "- on ajoute une colonne virtuelle au dataframe `machine_dataframe` via la méthode `.assign(day=day)`. Contrairement à la notation `df['nouvelle_colonne'] = valeur`, le dataframe n'est pas modifié, mais les méthodes qu'on applique sur le résultat -- et uniquement celles-ci -- tiennent compte de cette colonne virtuelle\n",
    "- noter que `day` n'est pas une valeur constante, mais une série, c'est-à-dire une liste de valeurs analogue à une colonne de dataframe (ce qui fait qu'on peut la lui assigner)\n",
    "\n",
    "Puis on écrit le fichier partitionné comme il faut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8364634-f97d-4ec4-b0fd-99eaf4835f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for machine_id, machine_dataframe in dataframes_per_machine.items():\n",
    "    day = machine_dataframe.timestamp.dt.strftime('%Y-%m-%d')\n",
    "    machine_dataframe.assign(day=day).to_parquet(f'data/ts_parquet_part/{machine_id}', index=False, partition_cols=[### CHANGE ME ###])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034f5e92-7a4d-4a3f-a2d9-44b90d7d9c8e",
   "metadata": {},
   "source": [
    "Vous pouvez aller consulter le répertoire `data/ts_parquet_part` pour voir comment Pandas (et son partenaire PyArrow) stockent les fichiers Parquet partitionnés.\n",
    "\n",
    "NB : si vous essayez plusieurs fois l'écriture partitionnée, les fichiers vont s'accumuler ; il faudrait normalement les supprimer avant mais nous ne le faisons pas ici, par simplicité."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0d5dfa",
   "metadata": {},
   "source": [
    "# Un partitionnement plus fin\n",
    "\n",
    "Sauvegarder une nouvelle version des données, avec un partitionnement à plusieurs niveaux :\n",
    "- `year` (exemple : 2024)\n",
    "  - `month` (exemple : 01 ou 1)\n",
    "    - `day_in_month` (exemple : 29)\n",
    "\n",
    "Ecrire ces données dans un répertoire différent (ex. `data/ts_parquet_part_ymd`) pour ne pas mélanger avec les fichiers de la question précédente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ac70f8",
   "metadata": {},
   "source": [
    "# Partitionnement global\n",
    "\n",
    "Si vous avez le temps, concaténez tous les dataframes dans `dataframes_per_machine` en un seul gros dataframe, et sauvez-le avec le partitionnement suivant :\n",
    "- `site` (exemple : Naves)\n",
    "  - `machine_name` (exemple : Lego_A)\n",
    "    - `year` (exemple : 2024)\n",
    "      - `month` (exemple : 01 ou 1)\n",
    "        - `day_in_month` (exemple : 29)\n",
    "\n",
    "Comme précédemment, écrivez-le dans un nouveau répertoire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b798b",
   "metadata": {},
   "source": [
    "On peut vérifier que les données relues contiennent l'ensemble des partitions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea4f320",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet('### CHANGE ME ###')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb2f33f",
   "metadata": {},
   "source": [
    "# Influence de la compression\n",
    "\n",
    "Parquet est un format compressé, pour occuper moins de place et accélérer les lectures et écritures. On peut choisir plusieurs algorithmes de compression.\n",
    "\n",
    "Consulter la [documentation de Pandas](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html#pandas.DataFrame.to_parquet), trouver le paramètre de l'algorithme de compression, et en tester quelques-uns en vérifiant la taille du fichier à chaque fois.\n",
    "\n",
    "Tester avec un seul dataframe (par exemple la machine `NA1`), sans partitionnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6279385e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
